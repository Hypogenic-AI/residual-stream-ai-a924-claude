\begin{abstract}
Large language models produce text with distinctive stylistic markers---formal tone, hedging language, structured formatting, comprehensive coverage---that humans readily identify as ``sounding like AI.'' We investigate whether this composite quality is encoded as a linear direction in the residual stream of a transformer language model. Using contrastive activation analysis on 200 paired human and ChatGPT responses from the \hcthree dataset, we extract a difference-in-means direction from \qwen (3B parameters) and find that it classifies AI-generated versus human-written text with 97.5\% accuracy (AUC = 0.999) on held-out data. However, we discover that this direction is heavily confounded with text length: the cosine similarity between the AI direction and a text-length direction is 0.93, reflecting the systematic verbosity of ChatGPT responses ($2\times$ longer than human answers on average). After projecting out the length component, a residual ``AI style'' direction still achieves 85.5\% accuracy---well above chance---indicating that the model encodes genuine stylistic differences beyond verbosity. Causal steering experiments confirm that adding or subtracting this direction during generation shifts output style in the expected direction, though the effect is modest in a base model. Our results suggest that ``sounding like AI'' is not a clean unitary concept in activation space but rather a composite of length and style features, with length as the dominant component.
\end{abstract}
